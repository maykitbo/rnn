## Neural Network Introduction

[Link1](https://docs.google.com/presentation/d/1GLjpTvAiwDrdxKCJNQsqAxpb3jdtmdc1pxKJziW4B04/edit?usp=sharing) and [Link2](https://docs.google.com/presentation/d/1Q3HXIXY8zgspxHJ7AJJY7TdqDUdRWhb8MqzTTztiN-I/edit?usp=sharing) to the presentations

The following topics were covered in the lectures:
- MNIST digits
- Hidden layers intuition
- Non-linear activation
- Weights matrix
- Gradient descent
- Weights update
- Derivatives in computational graphs
- Chain rule
- Gradient formulas
- Backprop intuition
- Mini-batch technology

**Further readings and materials**

1) [Andrew Ng (classic ML)](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)
2) [Andrej Karpathy and Fei-Fei Li (deep learning)](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)
3) [Radoslav Neychev (NLP in Russian)](https://www.youtube.com/playlist?list=PL4_hYwCyhAvY7k32D65q3xJVo8X8dc3Ye)
4) The team we are inspired by in our work [girafe.ai](https://github.com/girafe-ai)