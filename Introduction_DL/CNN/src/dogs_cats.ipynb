{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "source: https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "INPUT_SIZE = (128, 128)\n",
    "\n",
    "images_dir = 'datasets/dogs_cats/train'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "CLASSES = 2\n",
    "\n",
    "def load_dataset():\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for image_file in os.listdir(images_dir):\n",
    "        try:\n",
    "            # Load and process image\n",
    "            img_path = os.path.join(images_dir, image_file)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = transform(image)\n",
    "            data.append(image)\n",
    "            labels.append(0 if image_file.startswith('cat') else 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {img_path}: {e}\")\n",
    "    \n",
    "    return torch.stack(data), torch.tensor(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# data = data * 2 - 1\n",
    "\n",
    "\n",
    "print(data)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "plt.hist(labels, bins=range(CLASSES+1), align='left', rwidth=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import show_random\n",
    "\n",
    "show_random(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import residual\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNv2(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(CNNv2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=13, stride=2, dilation=3, padding=5)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=7, stride=1, padding=3)\n",
    "        self.layer1 = residual.ResidualBlock(64, 64)\n",
    "        self.layer2 = residual.ResidualBlock(64, 128, stride=2,\n",
    "            downsample=nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, stride=2), nn.BatchNorm2d(128)))\n",
    "        self.layer3 = residual.ResidualBlock(128, 256, stride=2,\n",
    "            downsample=nn.Sequential(nn.Conv2d(128, 256, kernel_size=1, stride=2), nn.BatchNorm2d(256)))\n",
    "        # self.layer4 = residual.ResidualBlock(256, 512, stride=2,\n",
    "        #     downsample=nn.Sequential(nn.Conv2d(256, 512, kernel_size=1, stride=2), nn.BatchNorm2d(512)))\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=4, stride=1, dilation=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.layer4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "raw_model = CNNv2(CLASSES)\n",
    "\n",
    "raw_model.apply(residual.init_weights)\n",
    "\n",
    "print(raw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import apply_random_filters\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "BATCH = 8\n",
    "TRAIN_FRATION = 0.8 \n",
    "\n",
    "full_dataset = TensorDataset(data, labels)\n",
    "\n",
    "# Splitting the dataset into training and validation datasets\n",
    "train_size = int(TRAIN_FRATION * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_data, val_data = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "def transform_wrapper(image):\n",
    "    return apply_random_filters(image, probabilities=[0.3, 0.2, 0.3, 0.2])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_data, transform=transform_wrapper)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(val_data)  # No transformation for test data\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit import fit\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "LR = 0.0007\n",
    "EPOCH = 20\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(raw_model.parameters(), lr=LR)\n",
    "\n",
    "fit(raw_model, train_loader, test_loader,\n",
    "    criterion, optimizer, EPOCH, CLASSES, 'backup/dogs_cats/residual_cnn_raw_model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import residual\n",
    "\n",
    "\n",
    "class TransferCNN(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(TransferCNN, self).__init__()\n",
    "        # Copy layers from the pre-trained model\n",
    "        self.conv1 = pretrained_model.conv1\n",
    "        self.bn1 = pretrained_model.bn1\n",
    "        self.relu = pretrained_model.relu\n",
    "        self.maxpool = pretrained_model.maxpool\n",
    "        self.layer1 = pretrained_model.layer1\n",
    "        self.layer2 = pretrained_model.layer2\n",
    "        self.layer3 = pretrained_model.layer3\n",
    "        # self.layer4 = pretrained_model.layer4\n",
    "\n",
    "        # self.new_layer4 = residual.ResidualBlock(256, 512, stride=2,\n",
    "        #     downsample=nn.Sequential(nn.Conv2d(256, 512, kernel_size=1, stride=2), nn.BatchNorm2d(512)))\n",
    "        self.new_conv = nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1)\n",
    "        self.new_bn = nn.BatchNorm2d(512)\n",
    "        self.new_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.new_fc = nn.Linear(512, CLASSES)  # Adjust num_new_classes as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the same forward method until layer4\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "\n",
    "        # New layers for the new task\n",
    "        x = self.relu(self.new_bn(self.new_conv(x)))\n",
    "        x = self.new_avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.new_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = residual.CNNv1(10)\n",
    "pretrained_model.load_state_dict(torch.load('backup/cifar_10/model_1e5v2_epoch_0'))\n",
    "transfer_model = TransferCNN(pretrained_model)\n",
    "\n",
    "# transfer_model.apply(init_weights)\n",
    "# transfer_model.new_layer4.apply(residual.init_weights)\n",
    "transfer_model.new_conv.apply(residual.init_weights)\n",
    "transfer_model.new_bn.apply(residual.init_weights)  # Only if you want to initialize BatchNorm layers\n",
    "transfer_model.new_fc.apply(residual.init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 0.0003\n",
    "EPOCH = 16 \n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(raw_model.parameters(), lr=LR)\n",
    "\n",
    "fit(pretrained_model, train_loader, test_loader,\n",
    "    criterion, optimizer, EPOCH, CLASSES, 'backup/dogs_cats/residual_cnn_transfer_model_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
