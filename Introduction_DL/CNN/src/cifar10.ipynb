{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize data\n",
    "source: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "INPUT_SIZE = (128, 128)\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def load_cifar10_batch(file):\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "\n",
    "    images = data.reshape((len(data), 3, 32, 32)).transpose(0, 1, 2, 3)\n",
    "    return torch.tensor(images), torch.tensor(labels)\n",
    "\n",
    "\n",
    "def load_cifar10(dir):\n",
    "    # Load all the training batches\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        data, labels = load_cifar10_batch(os.path.join(dir, 'data_batch_{}'.format(i)))\n",
    "        training_data.append(data)\n",
    "        training_labels.append(labels)\n",
    "\n",
    "    train_data = torch.cat(training_data)\n",
    "    train_labels = torch.cat(training_labels)\n",
    "\n",
    "    # Load the test batch\n",
    "    test_data, test_labels = load_cifar10_batch(os.path.join(dir, 'test_batch'))\n",
    "\n",
    "    resize_transform = transforms.Resize(INPUT_SIZE)\n",
    "    train_data = torch.stack([resize_transform(img) for img in train_data])\n",
    "    test_data = torch.stack([resize_transform(img) for img in test_data])\n",
    "\n",
    "    train_data = train_data.float()\n",
    "    test_data = test_data.float()\n",
    "\n",
    "    train_data /= 255\n",
    "    train_data = train_data * 2 - 1\n",
    "    test_data /= 255\n",
    "    test_data = test_data * 2 - 1\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "# Assuming you have your data in 'datasets/cifar-10-batches-py/'\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10('datasets/cifar-10-batches-py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_labels, bins=range(11), align='left', rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_labels, bins=range(11), align='left', rwidth=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import show_random\n",
    "\n",
    "\n",
    "show_random(train_data, train_labels,\n",
    "    probabilities=[0.1, 0.2, 0.2, 0.2, 0.15, 0.15], print_filters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from augmentation import apply_random_filters\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "BATCH = 8\n",
    "\n",
    "def transform_wrapper(image):\n",
    "    return apply_random_filters(image, probabilities=[0.2, 0.2, 0.3, 0.3])\n",
    "\n",
    "train_dataset = CustomDataset(train_data, train_labels.long(), transform=transform_wrapper)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "# For the test dataset, usually, we don't apply augmentations\n",
    "test_dataset = CustomDataset(test_data, test_labels.long())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import residual\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the model\n",
    "model = residual.CNNv1(10)        \n",
    "\n",
    "\n",
    "model.apply(residual.init_weights)\n",
    "\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import callback as cb\n",
    "import torch.optim as optim\n",
    "from fit import fit\n",
    "\n",
    "\n",
    "LR = 0.0001\n",
    "EPOCH = 45\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Call the fit function\n",
    "fit(model, train_loader, test_loader, criterion, optimizer,\n",
    "    epochs=45, classes=10, backup_path='backup/cifar10/residual_cnn_v1', stopper=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
