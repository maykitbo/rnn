## Recurrent neural networks

[Link](https://drive.google.com/file/d/11_Jw7L40fGFt3hVmWcwK8_TcN0SP78gS/view?usp=sharing) to the presentation

The following topics were covered in the lecture:
- Tokens and RNNs generating
- Contex and Bag-of-Words
- Simple RNN Example
- RNN text generation
- Vanilla RNN
- Long short-term memory (LSTM) and its variants (Peephole Connections, Coupled Forget and Input Gates)
- Gated Recurrent Unit (GRU)
- LSTM vs GRU
- Bidirectional RNNs
- Multi-layer RNNs

**Further readings and materials**

1) The team we are inspired by in our work [girafe.ai](https://github.com/girafe-ai)
2) [CS224N: Natural Language Processing with Deep Learning. Stanford](https://web.stanford.edu/class/cs224n/)
3) [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
4) [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)